{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import heartpy as hp\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pywt as pw\n",
    "import openpyxl\n",
    "import pickle\n",
    "import tsfel\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_participants = 23\n",
    "num_of_clips = 18\n",
    "\n",
    "#In Hz:\n",
    "cutoff = [0.5, 45]\n",
    "sample_rate = 256.0\n",
    "order = 3\n",
    "filtertype ='bandpass'\n",
    "def data_filter (data):\n",
    "    # Separate data by channel\n",
    "    for participant in data.keys():\n",
    "        #print(\"Starting \" + participant)\n",
    "        for clip_num in range(0, num_of_clips):\n",
    "            # Filter both channels from baseline\n",
    "            CH1 = [row[0] for row in data[participant]['ECG']['baseline'][clip_num]]\n",
    "            CH1_filtered = hp.filter_signal(CH1, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH1_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            CH2 = [row[1] for row in data[participant]['ECG']['baseline'][clip_num]]\n",
    "            CH2_filtered = hp.filter_signal(CH2, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH2_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            s_clip = \"clip \" + str(clip_num+1) \n",
    "\n",
    "            data[participant]['ECG'][s_clip] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH1'] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH1']['baseline'] = CH1_filtered.tolist()\n",
    "            data[participant]['ECG'][s_clip]['CH2'] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH2']['baseline'] = CH2_filtered.tolist()\n",
    "\n",
    "            \n",
    "            # Filter both channels from stimuli\n",
    "            CH1 = [row[0] for row in data[participant]['ECG']['stimuli'][clip_num]]\n",
    "            CH1_filtered = hp.filter_signal(CH1, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH1_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            CH2 = [row[1] for row in data[participant]['ECG']['stimuli'][clip_num]]\n",
    "            CH2_filtered = hp.filter_signal(CH2, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH2_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            data[participant]['ECG'][s_clip]['CH1']['stimuli'] = CH1_filtered.tolist()\n",
    "            data[participant]['ECG'][s_clip]['CH2']['stimuli'] = CH2_filtered.tolist()\n",
    "\n",
    "            data[participant]['ECG'][s_clip]['ScoreValence'] = data[participant]['ScoreValence'][clip_num]\n",
    "            data[participant]['ECG'][s_clip]['ScoreArousal'] = data[participant]['ScoreArousal'][clip_num]\n",
    "            data[participant]['ECG'][s_clip]['ScoreDominance'] = data[participant]['ScoreDominance'][clip_num]\n",
    "            \n",
    "            print(\"Done filtering raw for \" + participant + \" at clip#\" + str(clip_num+1) , end=\"\\r\", flush=True)\n",
    "        data[participant]['ECG'].pop('stimuli')\n",
    "        data[participant]['ECG'].pop('baseline')\n",
    "\n",
    "        data[participant].pop('ScoreValence')\n",
    "        data[participant].pop('ScoreArousal')\n",
    "        data[participant].pop('ScoreDominance')\n",
    "    \n",
    "    print(\"All done.... Highpass filter applied, cutoff: [\" + str(cutoff[0]) + \",\" + str(cutoff[1]) + \"]Hz\")\n",
    "    return data\n",
    "            \n",
    "            \n",
    "### Function to display graph\n",
    "def show_graph(participant, clip, dtype, range_min = 0, range_max = -1):\n",
    "    plt.plot(filtered_data[participant]['ECG'][dtype][clip][range_min:range_max])\n",
    "    plt.ylabel(\"mV\")\n",
    "    plt.xlabel(\"Sample #\")\n",
    "    graph_title = participant + \"  clip#\" + str(clip) + \" \" + dtype + \" data range: \" + str(range_min) + \" => \" + str(range_max)\n",
    "    plt.title(graph_title)\n",
    "\n",
    "### Function to split channels\n",
    "def chan_split(filtered_data, participant, clip, dtype):\n",
    "    ch1 = [row[0] for row in filtered_data[participant]['ECG'][dtype][clip]]\n",
    "    ch2 = [row[1] for row in filtered_data[participant]['ECG'][dtype][clip]]\n",
    "    \n",
    "    return ch1, ch2\n",
    "\n",
    "\n",
    "### Function to plot Heard Rate Signal Peak Detection\n",
    "def peak_detection(participant, clip, dtype, ch = 1, range_min = 0, range_max = -1):\n",
    "    \n",
    "    p1_c1_ch1, p1_c1_ch2 =(chan_split(participant, clip, dtype))\n",
    "    wd1 = hp.process(p1_c1_ch1[range_min:range_max], 256)\n",
    "    wd2 = hp.process(p1_c1_ch1[range_min:range_max], 256)\n",
    "    if ch == 1:\n",
    "        hp.plotter(wd1[0], wd1[1])\n",
    "    else:\n",
    "        hp.plotter(wd2[0], wd2[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Process RAW Data, then create and save processed data to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {}\n",
    "load_raw_from = '../data_set_raw_ECG/raw_data_dict.pkl'\n",
    "dump_to = '../data_set_raw_ECG/processed_data_dict.pkl'\n",
    "\n",
    "with open(load_raw_from, 'rb') as f:\n",
    "    raw_data = pickle.load(f)\n",
    "\n",
    "filtered_data = data_filter(raw_data)\n",
    "\n",
    "with open(dump_to, 'wb') as f:\n",
    "    pickle.dump(filtered_data, f)\n",
    "    print(\"Dumped proccesed data for \" , str(num_of_participants), \" participants and \", str(num_of_clips), \" \", dump_to)\n",
    "\n",
    "plt.plot(raw_data['participant_1']['ECG']['clip 1']['CH1']['stimuli'][0:1024])\n",
    "\n",
    "del raw_data\n",
    "del filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "load_processed_from = '../data_set_raw_ECG/processed_data_dict.pkl'\n",
    "with open(load_processed_from, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['participant_1']['ECG']['clip 1'].keys()\n",
    "len(data['participant_1']['ECG']['clip 1']['CH1']['stimuli'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_size = 512\n",
    "\n",
    "def extract_time_domain(processed_data):\n",
    "    features = {}\n",
    "    cfg = tsfel.get_features_by_domain()\n",
    "    for participant in processed_data:\n",
    "        features[participant] = {}\n",
    "        for clip in processed_data[participant]['ECG']:\n",
    "            mod = 256 % data[participant]['ECG'][clip]['CH1']['stimuli']\n",
    "            sample_size = len(data['participant_1']['ECG']['clip 1']['CH1']['stimuli']) \n",
    "            exclude = ((sample_size-(mod*256))/2)\n",
    "            i = 0\n",
    "            while i < sample_size :\n",
    "                features[participant][clip] = {}\n",
    "                features[participant][str(clip)]['CH1'] = tsfel.time_series_features_extractor(cfg, processed_data[participant]['ECG'][clip]['CH1']['stimuli'][i:i+windows_size])\n",
    "                features[participant][str(clip)]['CH2'] = tsfel.time_series_features_extractor(cfg, processed_data[participant]['ECG'][clip]['CH2']['stimuli'][i:i+windows_size])\n",
    "                \n",
    "                features[participant][str(clip)]['ScoreValence'] = data[participant]['ScoreValence']\n",
    "                features[participant][str(clip)]['ScoreArousal'] = data[participant]['ScoreArousal']\n",
    "                features[participant][str(clip)]['ScoreDominance'] = data[participant]['ScoreDominance']\n",
    "                \n",
    "                i += windows_size\n",
    "            \n",
    "            print(\"Done with time features for participant \", participant, \" clip \", clip)\n",
    "    return features\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def extract_time_domain_as_pd_df(processed_data):\n",
    "    combined_data = pd.DataFrame()\n",
    "    cfg = tsfel.get_features_by_domain()\n",
    "    for CH in ['CH1', 'CH2']:\n",
    "        for participant in processed_data:\n",
    "            for clip in processed_data[participant]['ECG']:\n",
    "                data_size = len(processed_data[participant]['ECG'][clip][CH]['stimuli'])\n",
    "                for_iterations = windows_size % data_size\n",
    "                for i in range(0, data_size, windows_size):\n",
    "                    print(\"Processing \" + participant + \" \" + clip + \" \" + CH + \" window:\" + str(int(i/windows_size)) + \"/\" + str(int(data_size/windows_size)) + \"          \", end=\"\\r\", flush=True)\n",
    "                    features_df = pd.DataFrame()\n",
    "                    features_df = tsfel.time_series_features_extractor(cfg, processed_data[participant]['ECG'][clip][CH]['stimuli'][i:i+windows_size], verbose=False)\n",
    "                    features_df['ScoreValence'] = data[participant]['ECG'][clip]['ScoreValence']\n",
    "                    features_df['ScoreArousal'] = data[participant]['ECG'][clip]['ScoreArousal']\n",
    "                    features_df['ScoreDominance'] = data[participant]['ECG'][clip]['ScoreDominance']\n",
    "                \n",
    "                    combined_data = pd.concat([combined_data, features_df], ignore_index=True)\n",
    "                \n",
    "                    # print(\"Done with time features for participant \", participant, \" clip \", clip)\n",
    "    print()\n",
    "    print(\"Done extracting time domain features\")\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save to dict to .pkl\n",
    "# time_domain_features = extract_time_domain(data)\n",
    "\n",
    "# dump_to = '../features/time_domain_all_dict.pkl'\n",
    "# with open(dump_to, 'wb') as f:\n",
    "#     pickle.dump(time_domain_features, f)\n",
    "# del time_domain_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pandas df to .pkl\n",
    "\n",
    "time_domain_df = extract_time_domain_as_pd_df(data)\n",
    "\n",
    "dump_to = '../features/time_domain_df.pkl'\n",
    "print(\"Dumping to pickle file\")\n",
    "time_domain_df.to_pickle(dump_to)\n",
    "print(\"Dumping to excel file\")\n",
    "time_domain_df.to_excel('../features/time_domain_df.xlsx')\n",
    "print(\"Dumping to csv file\")\n",
    "time_domain_df.to_csv('../features/time_domain_df.csv')\n",
    "\n",
    "del time_domain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common to all:\n",
    "\n",
    "# Load data\n",
    "features_df = pd.read_pickle('../features/time_domain_df.pkl')\n",
    "features_df.fillna(0, inplace=True)\n",
    "\n",
    "# Split Data into X and Y\n",
    "X = features_df.drop(['ScoreValence', 'ScoreArousal', 'ScoreDominance'], axis=1)\n",
    "Y = features_df[['ScoreValence', 'ScoreArousal', 'ScoreDominance']]  # Ensure correct DataFrame is used\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with 'linear' kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "svm_linear_model = SVC(kernel='linear')  # Correctly named SVM model\n",
    "multi_target_svm = MultiOutputClassifier(svm_linear_model)\n",
    "multi_target_svm.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_svm.predict(X_test)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with 'rbf' kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "svm_rbf_model = SVC(kernel='rbf')  # Correctly named SVM model\n",
    "multi_target_svm = MultiOutputClassifier(svm_rbf_model)\n",
    "multi_target_svm.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_svm.predict(X_test)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with 'poly' kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "svm_poly_model = SVC(kernel='poly')  # Correctly named SVM model\n",
    "multi_target_svm = MultiOutputClassifier(svm_poly_model)\n",
    "multi_target_svm.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_svm.predict(X_test)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PCA and 'rbf' kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define PCA model to use\n",
    "pca = PCA(0.95) #PCA with 95% variance\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train) #PCA on train data\n",
    "X_test_pca = pca.transform(X_test)#PCA on test data\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Train the SVM\n",
    "svm_pca_rbf_model = SVC(kernel='rbf', C=100, gamma=1, verbose=True)\n",
    "multi_target_svm = MultiOutputClassifier(svm_pca_rbf_model)\n",
    "multi_target_svm.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_svm.predict(X_test_pca)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_pca)\n",
    "X_test_scaled = scaler.transform(X_test_pca)\n",
    "\n",
    "# Multi-Output SVM Classifier\n",
    "# Best parameters: {'estimator__C': 100, 'estimator__gamma': 1, 'estimator__kernel': 'rbf'}\n",
    "svm_rbf_model = SVC(kernel='rbf', C=100, gamma=1)  # Base SVM model\n",
    "multi_target_svm = MultiOutputClassifier(svm_rbf_model)  # Multi-output wrapper\n",
    "\n",
    "# Training\n",
    "multi_target_svm.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = multi_target_svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    print(f\"Classification Report for {target}:\")\n",
    "    print(classification_report(Y_test.iloc[:, i], y_pred[:, i]))\n",
    "\n",
    "# Grid Search (Note: This is tricky with multi-output classifiers)\n",
    "# For simplicity, the following code is a template for a single-output grid search\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10, 100],  # Note the 'estimator__' prefix\n",
    "    'estimator__gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'estimator__kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "grid_search = GridSearchCV(MultiOutputClassifier(SVC()), param_grid, refit=True, verbose=2)\n",
    "grid_search.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK THIS ONE ^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA model to use\n",
    "pca = PCA(0.95)  # PCA with 95% variance\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)  # PCA on train data\n",
    "X_test_pca = pca.transform(X_test)  # PCA on test data\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Train the Random Forest\n",
    "rf_model = RandomForestClassifier(verbose=True)  # Random Forest model\n",
    "multi_target_rf = MultiOutputClassifier(rf_model)\n",
    "multi_target_rf.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_rf.predict(X_test_pca)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC_values = np.arange(pca.n_components_) + 1\n",
    "# plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.ylabel('Variance Explained')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_explained = np.cumsum(pca.explained_variance_ratio_)/np.sum(pca.explained_variance_ratio_)\n",
    "# plt.plot(PC_values, percent_explained, 'o-', linewidth=2, color='blue')\n",
    "# plt.hlines(0.95,1,15)\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.ylabel('Percent Explained')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = features_df.corr()\n",
    "# plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "# sns.heatmap(matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn import svm\n",
    "# from sklearn import metrics\n",
    "\n",
    "# # defining parameter range\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#               'kernel': ['rbf', 'poly']} \n",
    "  \n",
    "# grid = GridSearchCV(multi_target_svm, param_grid, refit = True, verbose = 3) #Grid model definition\n",
    "  \n",
    "# grid.fit(X_train, Y_train) #fit the grid mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# features_df = pd.read_pickle('../features/time_domain_df.pkl')\n",
    "# features_df.fillna(0, inplace=True)\n",
    "\n",
    "# # Split Data into X and Y\n",
    "# X = features_df.drop(['ScoreValence', 'ScoreArousal', 'ScoreDominance'], axis=1)\n",
    "# Y = features_df[['ScoreValence', 'ScoreArousal', 'ScoreDominance']]  # Ensure correct DataFrame is used\n",
    "\n",
    "# # Normalize data\n",
    "# scaler = StandardScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "# # \n",
    "# # Train-test split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # pca = PCA(0.95) #PCA with 95% variance\n",
    "\n",
    "# # X_train = pca.fit_transform(X_train) #PCA on train data\n",
    "# # X_test = pca.transform(X_test)#PCA on test data\n",
    "\n",
    "# #define PCA model to use\n",
    "# n_components=100\n",
    "# pca = PCA(n_components=n_components)\n",
    "\n",
    "# #fit PCA model to data\n",
    "# pca_fit = pca.fit(features_df)\n",
    "\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# # print(\"PCA size: \", str(len(pca)))\n",
    "\n",
    "# # Train a separate SVM model for each target\n",
    "# for i, target in enumerate(['ScoreValence', 'ScoreArousal', 'ScoreDominance']):\n",
    "#     svm_model = SVC(kernel='rbf')  # Correctly named SVM model\n",
    "#     svm_model.fit(X_train, Y_train[target])\n",
    "#     Y_pred = svm_model.predict(X_test)\n",
    "#     accuracy = accuracy_score(Y_test[target], Y_pred)\n",
    "#     print(f\"Accuracy for {target}:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
