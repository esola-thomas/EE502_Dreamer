{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import cuml\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import heartpy as hp\n",
    "import cudf as pd\n",
    "import numpy as np \n",
    "import pywt as pw\n",
    "import openpyxl\n",
    "import pickle\n",
    "import tsfel\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_participants = 23\n",
    "num_of_clips = 18\n",
    "\n",
    "#In Hz:\n",
    "cutoff = [0.5, 45]\n",
    "sample_rate = 256.0\n",
    "order = 3\n",
    "filtertype ='bandpass'\n",
    "def data_filter (data):\n",
    "    # Separate data by channel\n",
    "    for participant in data.keys():\n",
    "        #print(\"Starting \" + participant)\n",
    "        for clip_num in range(0, num_of_clips):\n",
    "            # Filter both channels from baseline\n",
    "            CH1 = [row[0] for row in data[participant]['ECG']['baseline'][clip_num]]\n",
    "            CH1_filtered = hp.filter_signal(CH1, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH1_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            CH2 = [row[1] for row in data[participant]['ECG']['baseline'][clip_num]]\n",
    "            CH2_filtered = hp.filter_signal(CH2, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH2_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            s_clip = \"clip \" + str(clip_num+1) \n",
    "\n",
    "            data[participant]['ECG'][s_clip] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH1'] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH1']['baseline'] = CH1_filtered.tolist()\n",
    "            data[participant]['ECG'][s_clip]['CH2'] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH2']['baseline'] = CH2_filtered.tolist()\n",
    "\n",
    "            \n",
    "            # Filter both channels from stimuli\n",
    "            CH1 = [row[0] for row in data[participant]['ECG']['stimuli'][clip_num]]\n",
    "            CH1_filtered = hp.filter_signal(CH1, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH1_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            CH2 = [row[1] for row in data[participant]['ECG']['stimuli'][clip_num]]\n",
    "            CH2_filtered = hp.filter_signal(CH2, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH2_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            data[participant]['ECG'][s_clip]['CH1']['stimuli'] = CH1_filtered.tolist()\n",
    "            data[participant]['ECG'][s_clip]['CH2']['stimuli'] = CH2_filtered.tolist()\n",
    "\n",
    "            data[participant]['ECG'][s_clip]['ScoreValence'] = data[participant]['ScoreValence'][clip_num]\n",
    "            data[participant]['ECG'][s_clip]['ScoreArousal'] = data[participant]['ScoreArousal'][clip_num]\n",
    "            data[participant]['ECG'][s_clip]['ScoreDominance'] = data[participant]['ScoreDominance'][clip_num]\n",
    "            \n",
    "            print(\"Done filtering raw for \" + participant + \" at clip#\" + str(clip_num+1) , end=\"\\r\", flush=True)\n",
    "        data[participant]['ECG'].pop('stimuli')\n",
    "        data[participant]['ECG'].pop('baseline')\n",
    "\n",
    "        data[participant].pop('ScoreValence')\n",
    "        data[participant].pop('ScoreArousal')\n",
    "        data[participant].pop('ScoreDominance')\n",
    "    \n",
    "    print(\"All done.... Highpass filter applied, cutoff: [\" + str(cutoff[0]) + \",\" + str(cutoff[1]) + \"]Hz\")\n",
    "    return data\n",
    "            \n",
    "            \n",
    "### Function to display graph\n",
    "def show_graph(participant, clip, dtype, range_min = 0, range_max = -1):\n",
    "    plt.plot(filtered_data[participant]['ECG'][dtype][clip][range_min:range_max])\n",
    "    plt.ylabel(\"mV\")\n",
    "    plt.xlabel(\"Sample #\")\n",
    "    graph_title = participant + \"  clip#\" + str(clip) + \" \" + dtype + \" data range: \" + str(range_min) + \" => \" + str(range_max)\n",
    "    plt.title(graph_title)\n",
    "\n",
    "### Function to split channels\n",
    "def chan_split(filtered_data, participant, clip, dtype):\n",
    "    ch1 = [row[0] for row in filtered_data[participant]['ECG'][dtype][clip]]\n",
    "    ch2 = [row[1] for row in filtered_data[participant]['ECG'][dtype][clip]]\n",
    "    \n",
    "    return ch1, ch2\n",
    "\n",
    "\n",
    "### Function to plot Heard Rate Signal Peak Detection\n",
    "def peak_detection(participant, clip, dtype, ch = 1, range_min = 0, range_max = -1):\n",
    "    \n",
    "    p1_c1_ch1, p1_c1_ch2 =(chan_split(participant, clip, dtype))\n",
    "    wd1 = hp.process(p1_c1_ch1[range_min:range_max], 256)\n",
    "    wd2 = hp.process(p1_c1_ch1[range_min:range_max], 256)\n",
    "    if ch == 1:\n",
    "        hp.plotter(wd1[0], wd1[1])\n",
    "    else:\n",
    "        hp.plotter(wd2[0], wd2[1])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Process RAW Data, then create and save processed data to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {}\n",
    "load_raw_from = '../data_set_raw_ECG/raw_data_dict.pkl'\n",
    "dump_to = '../data_set_raw_ECG/processed_data_dict.pkl'\n",
    "\n",
    "with open(load_raw_from, 'rb') as f:\n",
    "    raw_data = pickle.load(f)\n",
    "\n",
    "filtered_data = data_filter(raw_data)\n",
    "\n",
    "with open(dump_to, 'wb') as f:\n",
    "    pickle.dump(filtered_data, f)\n",
    "    print(\"Dumped proccesed data for \" , str(num_of_participants), \" participants and \", str(num_of_clips), \" \", dump_to)\n",
    "\n",
    "plt.plot(raw_data['participant_1']['ECG']['clip 1']['CH1']['stimuli'][0:1024])\n",
    "\n",
    "del raw_data\n",
    "del filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "load_processed_from = '../data_set_raw_ECG/processed_data_dict.pkl'\n",
    "with open(load_processed_from, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['participant_1']['ECG']['clip 1'].keys()\n",
    "len(data['participant_1']['ECG']['clip 1']['CH1']['stimuli'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Time Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_size = 512\n",
    "\n",
    "# TODO After extracting features check for outlayers\n",
    "\n",
    "def extract_time_domain(processed_data):\n",
    "    features = {}\n",
    "    cfg = tsfel.get_features_by_domain()\n",
    "    for participant in processed_data:\n",
    "        features[participant] = {}\n",
    "        for clip in processed_data[participant]['ECG']:\n",
    "            mod = windows_size % data[participant]['ECG'][clip]['CH1']['stimuli']\n",
    "            sample_size = len(data['participant_1']['ECG']['clip 1']['CH1']['stimuli']) \n",
    "            exclude = ((sample_size-(mod*windows_size))/2)\n",
    "            i = 0\n",
    "            while i < sample_size :\n",
    "                features[participant][clip] = {}\n",
    "                features[participant][str(clip)]['CH1'] = tsfel.time_series_features_extractor(cfg, processed_data[participant]['ECG'][clip]['CH1']['stimuli'][i:i+windows_size])\n",
    "                features[participant][str(clip)]['CH2'] = tsfel.time_series_features_extractor(cfg, processed_data[participant]['ECG'][clip]['CH2']['stimuli'][i:i+windows_size])\n",
    "                \n",
    "                features[participant][str(clip)]['ScoreValence'] = data[participant]['ScoreValence']\n",
    "                features[participant][str(clip)]['ScoreArousal'] = data[participant]['ScoreArousal']\n",
    "                features[participant][str(clip)]['ScoreDominance'] = data[participant]['ScoreDominance']\n",
    "                \n",
    "                i += windows_size\n",
    "            \n",
    "            print(\"Done with time features for participant \", participant, \" clip \", clip)\n",
    "    return features\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def extract_time_domain_as_pd_df(processed_data):\n",
    "    combined_data = pd.DataFrame()\n",
    "    cfg = tsfel.get_features_by_domain()\n",
    "    for CH in ['CH1', 'CH2']:\n",
    "        for participant in processed_data:\n",
    "            for clip in processed_data[participant]['ECG']:\n",
    "                data_size = len(processed_data[participant]['ECG'][clip][CH]['stimuli'])\n",
    "                for_iterations = windows_size % data_size\n",
    "                for i in range(0, data_size, windows_size):\n",
    "                    print(\"Processing \" + participant + \" \" + clip + \" \" + CH + \" window:\" + str(int(i/windows_size)) + \"/\" + str(int(data_size/windows_size)) + \"          \", end=\"\\r\", flush=True)\n",
    "                    features_df = pd.DataFrame()\n",
    "                    features_df = tsfel.time_series_features_extractor(cfg, processed_data[participant]['ECG'][clip][CH]['stimuli'][i:i+windows_size], verbose=False)\n",
    "                    features_df['ScoreValence'] = data[participant]['ECG'][clip]['ScoreValence']\n",
    "                    features_df['ScoreArousal'] = data[participant]['ECG'][clip]['ScoreArousal']\n",
    "                    features_df['ScoreDominance'] = data[participant]['ECG'][clip]['ScoreDominance']\n",
    "                \n",
    "                    combined_data = pd.concat([combined_data, features_df], ignore_index=True)\n",
    "                \n",
    "                    # print(\"Done with time features for participant \", participant, \" clip \", clip)\n",
    "    print()\n",
    "    print(\"Done extracting time domain features\")\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save to dict to .pkl\n",
    "# time_domain_features = extract_time_domain(data)\n",
    "\n",
    "# dump_to = '../features/time_domain_all_dict.pkl'\n",
    "# with open(dump_to, 'wb') as f:\n",
    "#     pickle.dump(time_domain_features, f)\n",
    "# del time_domain_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pandas df to .pkl\n",
    "\n",
    "time_domain_df = extract_time_domain_as_pd_df(data)\n",
    "\n",
    "dump_to = '../features/time_domain_df.pkl'\n",
    "print(\"Dumping to pickle file\")\n",
    "time_domain_df.to_pickle(dump_to)\n",
    "print(\"Dumping to excel file\")\n",
    "time_domain_df.to_excel('../features/time_domain_df.xlsx')\n",
    "print(\"Dumping to csv file\")\n",
    "time_domain_df.to_csv('../features/time_domain_df.csv')\n",
    "\n",
    "del time_domain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cuml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common to all:\n",
    "\n",
    "# Load data\n",
    "features_df = pd.read_pickle('./workspace/features/time_domain_df.pkl')\n",
    "features_df.fillna(0, inplace=True)\n",
    "\n",
    "# Split Data into X and Y\n",
    "X = features_df.drop(['ScoreValence', 'ScoreArousal', 'ScoreDominance'], axis=1)\n",
    "Y = features_df[['ScoreValence', 'ScoreArousal', 'ScoreDominance']]  # Ensure correct DataFrame is used\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with 'linear' kernel (takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the SVM\n",
    "# svm_linear_model_v = cuml.LinearRegression()  # Correctly named SVM model\n",
    "# svm_linear_model_a = cuml.LinearRegression()  # Correctly named SVM model\n",
    "# svm_linear_model_d = cuml.LinearRegression()  # Correctly named SVM model\n",
    "\n",
    "# # multi_target_svm = MultiOutputClassifier(svm_rbf_model)\n",
    "# # multi_target_svm.fit(X_train, Y_train)\n",
    "# svm_linear_model_v.fit(X_train, Y_train.iloc[:, 0])\n",
    "# svm_linear_model_a.fit(X_train, Y_train.iloc[:, 1])\n",
    "# svm_linear_model_d.fit(X_train, Y_train.iloc[:, 2])\n",
    "\n",
    "# # Predict and evaluate\n",
    "# Y_pred_v = svm_linear_model_v.predict(X_test)\n",
    "# Y_pred_a = svm_linear_model_a.predict(X_test)\n",
    "# Y_pred_d = svm_linear_model_d.predict(X_test)\n",
    "\n",
    "# # Evaluate each target\n",
    "# accuracy_v = accuracy_score(Y_test.iloc[:, 0], Y_pred_v)\n",
    "# print(f\"Accuracy for valance\", accuracy_v)\n",
    "# accuracy_a = accuracy_score(Y_test.iloc[:, 1], Y_pred_a)\n",
    "# print(f\"Accuracy for valance\", accuracy_a)\n",
    "# accuracy_d = accuracy_score(Y_test.iloc[:, 2], Y_pred_d)\n",
    "# print(f\"Accuracy for valance\", accuracy_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with 'rbf' kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "svm_rbf_model_v = cuml.SVC(kernel='rbf')  # Correctly named SVM model\n",
    "svm_rbf_model_a = cuml.SVC(kernel='rbf')  # Correctly named SVM model\n",
    "svm_rbf_model_d = cuml.SVC(kernel='rbf')  # Correctly named SVM model\n",
    "# multi_target_svm = MultiOutputClassifier(svm_rbf_model)\n",
    "# multi_target_svm.fit(X_train, Y_train)\n",
    "svm_rbf_model_v.fit(X_train, Y_train.iloc[:, 0])\n",
    "svm_rbf_model_a.fit(X_train, Y_train.iloc[:, 1])\n",
    "svm_rbf_model_d.fit(X_train, Y_train.iloc[:, 2])\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred_v = svm_rbf_model_v.predict(X_test)\n",
    "Y_pred_a = svm_rbf_model_a.predict(X_test)\n",
    "Y_pred_d = svm_rbf_model_d.predict(X_test)\n",
    "\n",
    "# Evaluate each target\n",
    "accuracy_v = accuracy_score(Y_test.iloc[:, 0], Y_pred_v)\n",
    "print(f\"Accuracy for valance\", accuracy_v)\n",
    "accuracy_a = accuracy_score(Y_test.iloc[:, 1], Y_pred_a)\n",
    "print(f\"Accuracy for valance\", accuracy_a)\n",
    "accuracy_d = accuracy_score(Y_test.iloc[:, 2], Y_pred_d)\n",
    "print(f\"Accuracy for valance\", accuracy_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with 'poly' kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "svm_poly_model = SVC(kernel='poly')  # Correctly named SVM model\n",
    "multi_target_svm = MultiOutputClassifier(svm_poly_model)\n",
    "multi_target_svm.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_svm.predict(X_test)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PCA and 'rbf' kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17168565 0.12259045 0.08255218 0.03878733 0.03227414 0.0298982\n",
      " 0.02412926 0.02047706 0.01734783 0.01384178 0.01269846 0.01070654\n",
      " 0.00932218 0.00776575 0.00700405 0.00633838 0.0057925  0.00565506\n",
      " 0.00534591 0.00513317 0.00504739 0.00493344 0.00476955 0.00473574\n",
      " 0.00464306 0.00445283 0.0043079  0.00430362 0.00426217 0.00418909\n",
      " 0.00416083 0.00414532 0.00407746 0.00403296 0.00401856 0.00397152\n",
      " 0.00395059 0.00391281 0.00386136 0.00382467 0.00377962 0.0037428\n",
      " 0.00365521 0.00363816 0.00362442 0.00360288 0.00358396 0.00351331\n",
      " 0.00349686 0.00348084 0.00343807 0.00341947 0.00340117 0.00335938\n",
      " 0.00333686 0.0032915  0.0032814  0.0032598  0.00322953 0.00320637\n",
      " 0.00318333 0.00311679 0.00309767 0.00303088 0.003015   0.00300109\n",
      " 0.00299421 0.00293173 0.00289858 0.00285786 0.00282832 0.00280652\n",
      " 0.00277874 0.00273604 0.00271155 0.0026806  0.00267766 0.00265819\n",
      " 0.00258479 0.00256522 0.00255816 0.00250822 0.00248441 0.00246497\n",
      " 0.00244359 0.00240541 0.00237032 0.00236053 0.00228375 0.00220974\n",
      " 0.00219801 0.00215957 0.00208781 0.00208    0.00203734 0.00201049\n",
      " 0.00197355 0.00196956 0.00192298 0.00189234 0.00188973 0.00183281\n",
      " 0.00179516 0.00177894 0.00171832 0.00168574 0.00160476 0.00158202\n",
      " 0.00154496 0.00152769 0.00150832 0.00148668 0.00146857 0.00143433\n",
      " 0.00143135 0.00141784 0.00139428 0.00136787 0.00135954 0.00134286\n",
      " 0.00131938 0.00131061 0.00128657 0.00125373 0.00123811 0.00121055\n",
      " 0.00120217 0.00118947 0.00117947 0.00115645 0.00114338 0.00112803\n",
      " 0.00111791 0.00109825 0.0010903  0.00108338 0.00107146 0.00104395\n",
      " 0.00102468 0.00102072 0.00101603 0.00099567 0.00098469 0.00098243\n",
      " 0.00096074 0.00095214 0.00094895 0.0009428  0.0009344  0.0009281\n",
      " 0.00091849]\n",
      "Accuracy for valance 0.43663037783082026\n",
      "Accuracy for valance 0.4978168481108459\n",
      "Accuracy for valance 0.4994469348547476\n"
     ]
    }
   ],
   "source": [
    "#define PCA model to use\n",
    "pca = PCA(0.95) #PCA with 95% variance\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train) #PCA on train data\n",
    "X_test_pca = pca.transform(X_test)#PCA on test data\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Train the SVM\n",
    "svm_rbf_pca_model_v = cuml.SVC(kernel='rbf')  # Correctly named SVM model\n",
    "svm_rbf_pca_model_a = cuml.SVC(kernel='rbf')  # Correctly named SVM model\n",
    "svm_rbf_pca_model_d = cuml.SVC(kernel='rbf')  # Correctly named SVM model\n",
    "# multi_target_svm = MultiOutputClassifier(svm_rbf_model)\n",
    "# multi_target_svm.fit(X_train, Y_train)\n",
    "svm_rbf_pca_model_v.fit(X_train_pca, Y_train.iloc[:, 0])\n",
    "svm_rbf_pca_model_a.fit(X_train_pca, Y_train.iloc[:, 1])\n",
    "svm_rbf_pca_model_d.fit(X_train_pca, Y_train.iloc[:, 2])\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred_v = svm_rbf_pca_model_v.predict(X_test_pca)\n",
    "Y_pred_a = svm_rbf_pca_model_a.predict(X_test_pca)\n",
    "Y_pred_d = svm_rbf_pca_model_d.predict(X_test_pca)\n",
    "\n",
    "# Evaluate each target\n",
    "accuracy_v = accuracy_score(Y_test.iloc[:, 0], Y_pred_v)\n",
    "print(f\"Accuracy for valance\", accuracy_v)\n",
    "accuracy_a = accuracy_score(Y_test.iloc[:, 1], Y_pred_a)\n",
    "print(f\"Accuracy for valance\", accuracy_a)\n",
    "accuracy_d = accuracy_score(Y_test.iloc[:, 2], Y_pred_d)\n",
    "print(f\"Accuracy for valance\", accuracy_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [23:18:37.462706] SVC with the linear kernel can be much faster using the specialized solver provided by LinearSVC. Consider switching to LinearSVC if tranining takes too long.\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for the SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0, 100.0],\n",
    "    'gamma' : [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "}\n",
    "\n",
    "# Create SVM models for each target\n",
    "svm_rbf_pca_model_v = cuml.SVC()  # SVM model for valence\n",
    "svm_rbf_pca_model_a = cuml.SVC()  # SVM model for arousal\n",
    "svm_rbf_pca_model_d = cuml.SVC()  # SVM model for dominance\n",
    "\n",
    "# Create GridSearchCV objects for hyperparameter tuning\n",
    "grid_search_v = GridSearchCV(svm_rbf_pca_model_v, param_grid, cv=5)\n",
    "grid_search_a = GridSearchCV(svm_rbf_pca_model_a, param_grid, cv=5)\n",
    "grid_search_d = GridSearchCV(svm_rbf_pca_model_d, param_grid, cv=5)\n",
    "\n",
    "# Fit GridSearchCV objects to the PCA-transformed data\n",
    "grid_search_v.fit(X_train_pca, Y_train.iloc[:, 0])\n",
    "grid_search_a.fit(X_train_pca, Y_train.iloc[:, 1])\n",
    "grid_search_d.fit(X_train_pca, Y_train.iloc[:, 2])\n",
    "\n",
    "# Get the best SVM models\n",
    "best_model_v = grid_search_v.best_estimator_\n",
    "best_model_a = grid_search_a.best_estimator_\n",
    "best_model_d = grid_search_d.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred_v = best_model_v.predict(X_test_pca)\n",
    "Y_pred_a = best_model_a.predict(X_test_pca)\n",
    "Y_pred_d = best_model_d.predict(X_test_pca)\n",
    "\n",
    "# Evaluate each target\n",
    "accuracy_v = accuracy_score(Y_test.iloc[:, 0], Y_pred_v)\n",
    "accuracy_a = accuracy_score(Y_test.iloc[:, 1], Y_pred_a)\n",
    "accuracy_d = accuracy_score(Y_test.iloc[:, 2], Y_pred_d)\n",
    "\n",
    "print(\"Best hyperparameters for valence:\", grid_search_v.best_params_)\n",
    "print(f\"Accuracy for valence: {accuracy_v}\")\n",
    "\n",
    "print(\"Best hyperparameters for arousal:\", grid_search_a.best_params_)\n",
    "print(f\"Accuracy for arousal: {accuracy_a}\")\n",
    "\n",
    "print(\"Best hyperparameters for dominance:\", grid_search_d.best_params_)\n",
    "print(f\"Accuracy for dominance: {accuracy_d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK THIS ONE ^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA model to use\n",
    "pca = PCA(0.9)  # PCA with 95% variance\n",
    "\n",
    "# TODO Check this:\n",
    "X_train_pca = pca.fit_transform(X_train)  # PCA on train data\n",
    "X_test_pca = pca.transform(X_test)  # PCA on test data\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "percent_explained = np.cumsum(pca.explained_variance_ratio_)/np.sum(pca.explained_variance_ratio_)\n",
    "num_features = len(percent_explained) - np.count_nonzero(percent_explained - 0.9>0) + 1 \n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "# Train the Random Forest\n",
    "rf_model = RandomForestClassifier(verbose=True)  # Random Forest model\n",
    "multi_target_rf = MultiOutputClassifier(rf_model)\n",
    "multi_target_rf.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_rf.predict(X_test_pca)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [100, 200, 300],  # Example parameters\n",
    "    'estimator__max_depth': [None, 10, 20, 30, 40],\n",
    "    'estimator__min_samples_split': [2, 5],\n",
    "    'estimator__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create the base model\n",
    "rf_model = RandomForestClassifier(verbose=True)\n",
    "\n",
    "# Create the GridSearchCV model\n",
    "grid_search = GridSearchCV(MultiOutputClassifier(rf_model), param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_rf_model = RandomForestClassifier(**grid_search.best_params_['estimator'], verbose=True)\n",
    "multi_target_rf = MultiOutputClassifier(best_rf_model)\n",
    "multi_target_rf.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_rf.predict(X_test_pca)\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC_values = np.arange(pca.n_components_) + 1\n",
    "# plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.ylabel('Variance Explained')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_explained = np.cumsum(pca.explained_variance_ratio_)/np.sum(pca.explained_variance_ratio_)\n",
    "# plt.plot(PC_values, percent_explained, 'o-', linewidth=2, color='blue')\n",
    "# plt.hlines(0.95,1,15)\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.ylabel('Percent Explained')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = features_df.corr()\n",
    "# plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "# sns.heatmap(matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn import svm\n",
    "# from sklearn import metrics\n",
    "\n",
    "# # defining parameter range\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#               'kernel': ['rbf', 'poly']} \n",
    "  \n",
    "# grid = GridSearchCV(multi_target_svm, param_grid, refit = True, verbose = 3) #Grid model definition\n",
    "  \n",
    "# grid.fit(X_train, Y_train) #fit the grid mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# features_df = pd.read_pickle('../features/time_domain_df.pkl')\n",
    "# features_df.fillna(0, inplace=True)\n",
    "\n",
    "# # Split Data into X and Y\n",
    "# X = features_df.drop(['ScoreValence', 'ScoreArousal', 'ScoreDominance'], axis=1)\n",
    "# Y = features_df[['ScoreValence', 'ScoreArousal', 'ScoreDominance']]  # Ensure correct DataFrame is used\n",
    "\n",
    "# # Normalize data\n",
    "# scaler = StandardScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "# # \n",
    "# # Train-test split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # pca = PCA(0.95) #PCA with 95% variance\n",
    "\n",
    "# # X_train = pca.fit_transform(X_train) #PCA on train data\n",
    "# # X_test = pca.transform(X_test)#PCA on test data\n",
    "\n",
    "# #define PCA model to use\n",
    "# n_components=100\n",
    "# pca = PCA(n_components=n_components)\n",
    "\n",
    "# #fit PCA model to data\n",
    "# pca_fit = pca.fit(features_df)\n",
    "\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# # print(\"PCA size: \", str(len(pca)))\n",
    "\n",
    "# # Train a separate SVM model for each target\n",
    "# for i, target in enumerate(['ScoreValence', 'ScoreArousal', 'ScoreDominance']):\n",
    "#     svm_model = SVC(kernel='rbf')  # Correctly named SVM model\n",
    "#     svm_model.fit(X_train, Y_train[target])\n",
    "#     Y_pred = svm_model.predict(X_test)\n",
    "#     accuracy = accuracy_score(Y_test[target], Y_pred)\n",
    "#     print(f\"Accuracy for {target}:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
