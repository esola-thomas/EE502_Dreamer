{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "import heartpy as hp\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pywt as pw\n",
    "import openpyxl\n",
    "import pickle\n",
    "import tsfel\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_participants = 23\n",
    "num_of_clips = 18\n",
    "\n",
    "#In Hz:\n",
    "cutoff = [0.5, 45]\n",
    "sample_rate = 256.0\n",
    "order = 3\n",
    "filtertype ='bandpass'\n",
    "def data_filter (data):\n",
    "    # Separate data by channel\n",
    "    for participant in data.keys():\n",
    "        #print(\"Starting \" + participant)\n",
    "        for clip_num in range(0, num_of_clips):\n",
    "            # Filter both channels from baseline\n",
    "            CH1 = [row[0] for row in data[participant]['ECG']['baseline'][clip_num]]\n",
    "            CH1_filtered = hp.filter_signal(CH1, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH1_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            CH2 = [row[1] for row in data[participant]['ECG']['baseline'][clip_num]]\n",
    "            CH2_filtered = hp.filter_signal(CH2, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH2_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            s_clip = \"clip \" + str(clip_num+1) \n",
    "\n",
    "            data[participant]['ECG'][s_clip] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH1'] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH1']['baseline'] = CH1_filtered.tolist()\n",
    "            data[participant]['ECG'][s_clip]['CH2'] = {}\n",
    "            data[participant]['ECG'][s_clip]['CH2']['baseline'] = CH2_filtered.tolist()\n",
    "\n",
    "            \n",
    "            # Filter both channels from stimuli\n",
    "            CH1 = [row[0] for row in data[participant]['ECG']['stimuli'][clip_num]]\n",
    "            CH1_filtered = hp.filter_signal(CH1, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH1_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            CH2 = [row[1] for row in data[participant]['ECG']['stimuli'][clip_num]]\n",
    "            CH2_filtered = hp.filter_signal(CH2, cutoff = cutoff, sample_rate = sample_rate, order = order, filtertype=filtertype)\n",
    "            # CH2_filtered = hp.butter_lowpass_filter(data, cutoff=cutoff, sample_rate=sample_rate, order=order)\n",
    "\n",
    "            data[participant]['ECG'][s_clip]['CH1']['stimuli'] = CH1_filtered.tolist()\n",
    "            data[participant]['ECG'][s_clip]['CH2']['stimuli'] = CH2_filtered.tolist()\n",
    "\n",
    "            data[participant]['ECG'][s_clip]['ScoreValence'] = data[participant]['ScoreValence'][clip_num]\n",
    "            data[participant]['ECG'][s_clip]['ScoreArousal'] = data[participant]['ScoreArousal'][clip_num]\n",
    "            data[participant]['ECG'][s_clip]['ScoreDominance'] = data[participant]['ScoreDominance'][clip_num]\n",
    "            \n",
    "            print(\"Done filtering raw for \" + participant + \" at clip#\" + str(clip_num+1) + \"                 \", end=\"\\r\", flush=True)\n",
    "        data[participant]['ECG'].pop('stimuli')\n",
    "        data[participant]['ECG'].pop('baseline')\n",
    "\n",
    "        data[participant].pop('ScoreValence')\n",
    "        data[participant].pop('ScoreArousal')\n",
    "        data[participant].pop('ScoreDominance')\n",
    "    \n",
    "    print(\"All done.... Highpass filter applied, cutoff: [\" + str(cutoff[0]) + \",\" + str(cutoff[1]) + \"]Hz\")\n",
    "    return data\n",
    "            \n",
    "            \n",
    "### Function to display graph\n",
    "def show_graph(participant, clip, dtype, range_min = 0, range_max = -1):\n",
    "    plt.plot(filtered_data[participant]['ECG'][dtype][clip][range_min:range_max])\n",
    "    plt.ylabel(\"mV\")\n",
    "    plt.xlabel(\"Sample #\")\n",
    "    graph_title = participant + \"  clip#\" + str(clip) + \" \" + dtype + \" data range: \" + str(range_min) + \" => \" + str(range_max)\n",
    "    plt.title(graph_title)\n",
    "\n",
    "### Function to split channels\n",
    "def chan_split(filtered_data, participant, clip, dtype):\n",
    "    ch1 = [row[0] for row in filtered_data[participant]['ECG'][dtype][clip]]\n",
    "    ch2 = [row[1] for row in filtered_data[participant]['ECG'][dtype][clip]]\n",
    "    \n",
    "    return ch1, ch2\n",
    "\n",
    "\n",
    "### Function to plot Heard Rate Signal Peak Detection\n",
    "def peak_detection(participant, clip, dtype, ch = 1, range_min = 0, range_max = -1):\n",
    "    \n",
    "    p1_c1_ch1, p1_c1_ch2 =(chan_split(participant, clip, dtype))\n",
    "    wd1 = hp.process(p1_c1_ch1[range_min:range_max], 256)\n",
    "    wd2 = hp.process(p1_c1_ch1[range_min:range_max], 256)\n",
    "    if ch == 1:\n",
    "        hp.plotter(wd1[0], wd1[1])\n",
    "    else:\n",
    "        hp.plotter(wd2[0], wd2[1])\n",
    "\n",
    "def extract_time_domain_as_pd_df(processed_data):\n",
    "    combined_data = pd.DataFrame()\n",
    "    cfg = tsfel.get_features_by_domain()\n",
    "    for CH in ['CH1', 'CH2']:\n",
    "        for participant in processed_data:\n",
    "            for clip in processed_data[participant]['ECG']:\n",
    "                data_size = len(processed_data[participant]['ECG'][clip][CH]['stimuli'])\n",
    "                for_iterations = windows_size % data_size\n",
    "                for i in range(0, data_size, windows_size):\n",
    "                    print(\"Processing \" + participant + \" \" + clip + \" \" + CH + \" window:\" + str(int(i/windows_size)) + \"/\" + str(int(data_size/windows_size)) + \"          \", end=\"\\r\", flush=True)\n",
    "                    features_df = pd.DataFrame()\n",
    "                    features_df = tsfel.time_series_features_extractor(cfg, processed_data[participant]['ECG'][clip][CH]['stimuli'][i:i+windows_size], verbose=False)\n",
    "                    features_df['ScoreValence'] = data[participant]['ECG'][clip]['ScoreValence']\n",
    "                    features_df['ScoreArousal'] = data[participant]['ECG'][clip]['ScoreArousal']\n",
    "                    features_df['ScoreDominance'] = data[participant]['ECG'][clip]['ScoreDominance']\n",
    "                \n",
    "                    combined_data = pd.concat([combined_data, features_df], ignore_index=True)\n",
    "                \n",
    "                    # print(\"Done with time features for participant \", participant, \" clip \", clip)\n",
    "    print()\n",
    "    print(\"Done extracting time domain features\")\n",
    "    return combined_data\n",
    "\n",
    "def extract_ecg_features(processed_data):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for CH in ['CH1', 'CH2']:\n",
    "        for participant in processed_data:\n",
    "            for clip in processed_data[participant]['ECG']:\n",
    "                print(\"Processing \" + participant + \" \" + clip + \" \" + CH + \"          \", end=\"\\r\", flush=True)\n",
    "                features_df = pd.DataFrame()\n",
    "                hp_tupple = hp.process(processed_data[participant]['ECG'][clip][CH]['stimuli'], 256)\n",
    "                features_df = pd.DataFrame([hp_tupple[1]])\n",
    "                features_df['ScoreValence'] = processed_data[participant]['ECG'][clip]['ScoreValence']\n",
    "                features_df['ScoreArousal'] = processed_data[participant]['ECG'][clip]['ScoreArousal']\n",
    "                features_df['ScoreDominance'] = processed_data[participant]['ECG'][clip]['ScoreDominance']\n",
    "            \n",
    "                combined_data = pd.concat([combined_data, features_df], ignore_index=True)\n",
    "            \n",
    "                # print(\"Done with time features for participant \", participant, \" clip \", clip)\n",
    "    print()\n",
    "    print(\"Done extracting time domain features\")\n",
    "    return combined_data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Process RAW Data, then create and save processed data to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {}\n",
    "load_raw_from = '../data_set_raw_ECG/raw_data_dict.pkl'\n",
    "\n",
    "with open(load_raw_from, 'rb') as f:\n",
    "    raw_data = pickle.load(f)\n",
    "\n",
    "filtered_data = data_filter(raw_data)\n",
    "\n",
    "plt.plot(filtered_data['participant_1']['ECG']['clip 1']['CH1']['stimuli'][0:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = extract_ecg_features(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = basic_features.drop(['ScoreValence', 'ScoreArousal', 'ScoreDominance'], axis=1)\n",
    "Y = basic_features[['ScoreValence', 'ScoreArousal', 'ScoreDominance']]  # Ensure correct DataFrame is used\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM\n",
    "svm_rbf_model = SVC(kernel='rbf', C=100, gamma=1)  # Correctly named SVM model\n",
    "multi_target_svm = MultiOutputClassifier(svm_rbf_model)\n",
    "multi_target_svm.fit(X_train, Y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "Y_pred = multi_target_svm.predict(X_test)\n",
    "\n",
    "# Evaluate each target\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    accuracy = accuracy_score(Y_test.iloc[:, i], Y_pred[:, i])\n",
    "    print(f\"Accuracy for {target}:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS A GOOD OPTION TOO ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Multi-Output SVM Classifier\n",
    "# Best parameters: {'estimator__C': 100, 'estimator__gamma': 1, 'estimator__kernel': 'rbf'}\n",
    "svm_rbf_model = SVC(kernel='rbf', C=100, gamma=1)  # Base SVM model\n",
    "multi_target_svm = MultiOutputClassifier(svm_rbf_model)  # Multi-output wrapper\n",
    "\n",
    "# Training\n",
    "multi_target_svm.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = multi_target_svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "for i, target in enumerate(['Valence', 'Arousal', 'Dominance']):\n",
    "    print(f\"Classification Report for {target}:\")\n",
    "    print(classification_report(Y_test.iloc[:, i], y_pred[:, i]))\n",
    "\n",
    "# Grid Search (Note: This is tricky with multi-output classifiers)\n",
    "# For simplicity, the following code is a template for a single-output grid search\n",
    "param_grid = {\n",
    "    'estimator__C': [0.1, 1, 10, 100],  # Note the 'estimator__' prefix\n",
    "    'estimator__gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'estimator__kernel': ['rbf', 'linear']\n",
    "}\n",
    "grid_search = GridSearchCV(MultiOutputClassifier(SVC()), param_grid, refit=True, verbose=2)\n",
    "grid_search.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
